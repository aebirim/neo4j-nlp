{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/philgooch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to Neo4j instance using py2neo - default running locally\n",
    "graphdb = Graph('http://neo4j:pass@localhost:7474/db/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some parameterized Cypher queries\n",
    "\n",
    "# For data insertion\n",
    "INSERT_QUERY = '''\n",
    "    FOREACH (t IN {wordPairs} | \n",
    "        MERGE (w0:Word {word: t[0]})\n",
    "        MERGE (w1:Word {word: t[1]})\n",
    "        CREATE (w0)-[:NEXT_WORD]->(w1)\n",
    "        )\n",
    "'''\n",
    "\n",
    "# get the set of words that appear to the left of a specified word in the text corpus\n",
    "LEFT1_QUERY = '''\n",
    "    MATCH (s:Word {word: {word}})\n",
    "    MATCH (w:Word)-[:NEXT_WORD]->(s)\n",
    "    RETURN w.word as word\n",
    "'''\n",
    "\n",
    "# get the set of words that appear to the right of a specified word in the text corpus\n",
    "RIGHT1_QUERY = '''\n",
    "    MATCH (s:Word {word: {word}})\n",
    "    MATCH (w:Word)<-[:NEXT_WORD]-(s)\n",
    "    RETURN w.word as word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = re.compile('[%s’‘“”]' % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert a sentence string into a list of lists of adjacent word pairs\n",
    "# arrifySentence(\"Hi there, Bob!\") = [[\"hi\", \"there\"], [\"there\", \"bob\"]]\n",
    "def arrifySentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.strip()\n",
    "    sentence = PUNCTUATION.sub('', sentence)\n",
    "    wordArray = [word for word in sentence.split() if word not in stopwords.words('english')]\n",
    "    tupleList = []\n",
    "    for i, word in enumerate(wordArray):\n",
    "        if i+1 == len(wordArray):\n",
    "            break\n",
    "        tupleList.append([word, wordArray[i+1]])\n",
    "    return tupleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFile(filepath):\n",
    "    tx = graphdb.begin()\n",
    "    with open(filepath, \"r\") as f:\n",
    "        count = 0\n",
    "        for l in f:\n",
    "            params = {'wordPairs': arrifySentence(l)}\n",
    "            tx.run(INSERT_QUERY, params)\n",
    "            tx.process()\n",
    "            count += 1\n",
    "            if count > 300:\n",
    "                tx.commit()\n",
    "                tx = graphdb.begin()\n",
    "                count = 0\n",
    "    f.close()\n",
    "    tx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadFile('resources/scandal-full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a set of all words that appear to the left of `word`\n",
    "def left1(word):\n",
    "    params = {\n",
    "        'word': word.lower()\n",
    "    }\n",
    "    tx = graphdb.begin()\n",
    "    results = tx.run(LEFT1_QUERY, params)\n",
    "    tx.commit()\n",
    "    words = []\n",
    "    for result in results:\n",
    "        for line in result:\n",
    "            words.append(line)\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a set of all words that appear to the right of `word`\n",
    "def right1(word):\n",
    "    params = {\n",
    "        'word': word.lower()\n",
    "    }\n",
    "    tx = graphdb.begin()\n",
    "    results = tx.run(RIGHT1_QUERY, params)\n",
    "    tx.commit()\n",
    "    words = []\n",
    "    for result in results:\n",
    "        for line in result:\n",
    "            words.append(line)\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute Jaccard coefficient\n",
    "def jaccard(a,b):\n",
    "    intSize = len(a.intersection(b))\n",
    "    unionSize = len(a.union(b))\n",
    "    return intSize / unionSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we define paradigmatic similarity as the average of the Jaccard coefficents of the `left1` and `right1` sets\n",
    "def paradigSimilarity(w1, w2):\n",
    "    return (jaccard(left1(w1), left1(w2)) + jaccard(right1(w1), right1(w2))) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paradigSimilarity('king', 'kingdom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041245791245791245"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paradigSimilarity('woman', 'photograph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023809523809523808"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paradigSimilarity('holmes', 'bohemia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
